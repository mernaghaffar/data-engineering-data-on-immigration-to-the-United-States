{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project, I'll be looking at the immigration data for the US. More specifically, looking into:\n",
    "\n",
    "- the seasonality of travel\n",
    "- the connection between the volume of travel and the number of entry ports (ie airports)\n",
    "- the connection between the volume of travel and the demographics of various cities\n",
    "\n",
    "To accomplish this, the following datasets will be used:\n",
    "\n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office and includes the contents of the i94 form on entry to the united states. A data dictionary is included in the workspace.\n",
    "\n",
    "2. countries.csv : table containing country codes used in the dataset, extracted from the data dictionary\n",
    "3. i94portCodes.csv: table containing city codes used in the dataset, extracted from the data dictionary\n",
    "\n",
    "\n",
    "4. World Temperature Data: This dataset comes from Kaggle and includes the temperatures of various cities in the world fomr 1743 to 2013.\n",
    "\n",
    "5. U.S. City Demographic Data: This data comes from OpenSoft. It contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000 and comes from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "6. Airport Code Table: This is a simple table of airport codes and corresponding cities.\n",
    "\n",
    "The data: \n",
    "\n",
    "1. is aggregated based on time (year, month, day, etc...)\n",
    "2. is aggregated data by cities and airports\n",
    "3.  was looked into for the impact of temperatures on the in and ouflux of travelers\n",
    "4.  was looked into the impact on regional demographics\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "    Step 1: Scope the Project and Gather Data\n",
    "    Step 2: Explore and Assess the Data\n",
    "    Step 3: Define the Data Model\n",
    "    Step 4: Run ETL to Model the Data\n",
    "    Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Do all imports and installs here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, date_add\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748522.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.498180e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497969e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748524.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497975e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748525.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497325e+10</td>\n",
       "      <td>00028</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748526.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.501355e+10</td>\n",
       "      <td>00002</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1  5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2  5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3  5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4  5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "5  5748522.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "6  5748523.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "7  5748524.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "8  5748525.0  2016.0     4.0   245.0   464.0     HOU  20574.0      1.0   \n",
       "9  5748526.0  2016.0     4.0   245.0   464.0     LOS  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "0      CA  20582.0   ...        None        M   1976.0  10292016      F   \n",
       "1      NV  20591.0   ...        None        M   1984.0  10292016      F   \n",
       "2      WA  20582.0   ...        None        M   1987.0  10292016      M   \n",
       "3      WA  20588.0   ...        None        M   1987.0  10292016      F   \n",
       "4      WA  20588.0   ...        None        M   1988.0  10292016      M   \n",
       "5      HI  20579.0   ...        None        M   1959.0  10292016      M   \n",
       "6      HI  20586.0   ...        None        M   1950.0  10292016      F   \n",
       "7      HI  20586.0   ...        None        M   1975.0  10292016      F   \n",
       "8      FL  20581.0   ...        None        M   1989.0  10292016      M   \n",
       "9      CA  20581.0   ...        None        M   1990.0  10292016      F   \n",
       "\n",
       "  insnum airline        admnum  fltno visatype  \n",
       "0   None      QF  9.495387e+10  00011       B1  \n",
       "1   None      VA  9.495562e+10  00007       B1  \n",
       "2   None      DL  9.495641e+10  00040       B1  \n",
       "3   None      DL  9.495645e+10  00040       B1  \n",
       "4   None      DL  9.495639e+10  00040       B1  \n",
       "5   None      NZ  9.498180e+10  00010       B2  \n",
       "6   None      NZ  9.497969e+10  00010       B2  \n",
       "7   None      NZ  9.497975e+10  00010       B2  \n",
       "8   None      NZ  9.497325e+10  00028       B2  \n",
       "9   None      NZ  9.501355e+10  00002       B2  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_data = spark.read.parquet(\"sas_data\")\n",
    "print(imm_data.count())\n",
    "imm_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## U.S City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_dem_data = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "print(city_dem_data.info())\n",
    "city_dem_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airport Code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_code_data = pd.read_csv('airport-codes_csv.csv')\n",
    "print(airport_code_data.info())\n",
    "airport_code_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = pd.read_csv('GlobalLandTemperaturesByCity.csv')\n",
    "print(temp_data.info())\n",
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_code_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_code_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iso_country\n",
       "AD        2\n",
       "AE       57\n",
       "AF       64\n",
       "AG        3\n",
       "AI        1\n",
       "AL       13\n",
       "AM       13\n",
       "AO      104\n",
       "AQ       27\n",
       "AR      848\n",
       "AS        4\n",
       "AT      145\n",
       "AU     1963\n",
       "AW        1\n",
       "AZ       35\n",
       "BA       15\n",
       "BB        6\n",
       "BD       16\n",
       "BE      146\n",
       "BF       51\n",
       "BG      134\n",
       "BH        4\n",
       "BI        7\n",
       "BJ       10\n",
       "BL        1\n",
       "BM        3\n",
       "BN        2\n",
       "BO      197\n",
       "BQ        3\n",
       "BR     4334\n",
       "      ...  \n",
       "TM       21\n",
       "TN       15\n",
       "TO        6\n",
       "TR      124\n",
       "TT        3\n",
       "TV        3\n",
       "TW       65\n",
       "TZ      207\n",
       "UA      191\n",
       "UG       38\n",
       "UM        6\n",
       "US    22757\n",
       "UY       54\n",
       "UZ      176\n",
       "VA        1\n",
       "VC        6\n",
       "VE      592\n",
       "VG        3\n",
       "VI        9\n",
       "VN       50\n",
       "VU       32\n",
       "WF        2\n",
       "WS        4\n",
       "XK        6\n",
       "YE       25\n",
       "YT        1\n",
       "ZA      489\n",
       "ZM      103\n",
       "ZW      138\n",
       "ZZ        7\n",
       "Name: iso_country, Length: 243, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the countries where the airports are located\n",
    "airport_code_data.groupby('iso_country')['iso_country'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# limiting our data to only country Canada since our immagration dataset only contains those entries\n",
    "airport_code_data = airport_code_data[airport_code_data['iso_country'].fillna('').str.upper().str.contains('US')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "balloonport          18\n",
       "closed             1326\n",
       "heliport           6265\n",
       "large_airport       170\n",
       "medium_airport      692\n",
       "seaplane_base       566\n",
       "small_airport     13720\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at the types column\n",
    "airport_code_data.groupby('type')['type'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Since we are looking at immigration data, no data will be collected from closed airport (we're assuming its closed) nor from heliport and seaplane_baseand balloonport since those are used for recreational purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_code_data = airport_code_data[~airport_code_data['type'].str.strip().isin(['closed','seaplane_base', 'heliport', 'balloonport'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft       63\n",
       "continent       14582\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality       50\n",
       "gps_code          399\n",
       "iata_code       12717\n",
       "local_code        199\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for other null values\n",
    "airport_code_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7653</th>\n",
       "      <td>6XA4</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Zadow Airstrip</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6XA4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-95.954353809, 29.991738550900003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7887</th>\n",
       "      <td>74xa</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Gun Barrel City Airpark</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74XA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-96.1456650496, 32.3551499558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>79ID</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Kooskia (Clear Creek Int) Airport</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-115.869691372, 46.0488642914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8114</th>\n",
       "      <td>79WT</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ellensburg (Rotor Ranch) Airport</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79WT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-120.589778423, 47.091426059499994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9055</th>\n",
       "      <td>8FA4</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Samsula / Coe Field</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8FA4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-81.1328315735, 29.0102045831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ident           type                               name  elevation_ft  \\\n",
       "7653  6XA4  small_airport                     Zadow Airstrip           NaN   \n",
       "7887  74xa  small_airport            Gun Barrel City Airpark         385.0   \n",
       "8082  79ID  small_airport  Kooskia (Clear Creek Int) Airport        1800.0   \n",
       "8114  79WT  small_airport   Ellensburg (Rotor Ranch) Airport        1962.0   \n",
       "9055  8FA4  small_airport                Samsula / Coe Field          40.0   \n",
       "\n",
       "     continent iso_country iso_region municipality gps_code iata_code  \\\n",
       "7653       NaN          US      US-TX          NaN     6XA4       NaN   \n",
       "7887       NaN          US      US-TX          NaN     74XA       NaN   \n",
       "8082       NaN          US      US-ID          NaN     79ID       NaN   \n",
       "8114       NaN          US      US-WA          NaN     79WT       NaN   \n",
       "9055       NaN          US      US-FL          NaN     8FA4       NaN   \n",
       "\n",
       "     local_code                         coordinates  \n",
       "7653        NaN   -95.954353809, 29.991738550900003  \n",
       "7887        NaN       -96.1456650496, 32.3551499558  \n",
       "8082        NaN       -115.869691372, 46.0488642914  \n",
       "8114        NaN  -120.589778423, 47.091426059499994  \n",
       "9055        NaN       -81.1328315735, 29.0102045831  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if we can use municipality to merge our dataset\n",
    "airport_code_data[airport_code_data.municipality.isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#removing those with lack of information (mostly municipality as null) from our dataset\n",
    "airport_code_data = airport_code_data[~airport_code_data['municipality'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#to join with the other dataset, need municipality column to be upper case\n",
    "airport_code_data.municipality = airport_code_data.municipality.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iso_region\n",
       "US-AK      586\n",
       "US-AL      197\n",
       "US-AR      291\n",
       "US-AZ      214\n",
       "US-CA      551\n",
       "US-CO      288\n",
       "US-CT       56\n",
       "US-DC        2\n",
       "US-DE       36\n",
       "US-FL      522\n",
       "US-GA      365\n",
       "US-HI       35\n",
       "US-IA      232\n",
       "US-ID      238\n",
       "US-IL      579\n",
       "US-IN      486\n",
       "US-KS      372\n",
       "US-KY      164\n",
       "US-LA      281\n",
       "US-MA       79\n",
       "US-MD      157\n",
       "US-ME      122\n",
       "US-MI      379\n",
       "US-MN      361\n",
       "US-MO      411\n",
       "US-MS      211\n",
       "US-MT      255\n",
       "US-NC      349\n",
       "US-ND      297\n",
       "US-NE      259\n",
       "US-NH       54\n",
       "US-NJ      116\n",
       "US-NM      149\n",
       "US-NV      113\n",
       "US-NY      402\n",
       "US-OH      492\n",
       "US-OK      372\n",
       "US-OR      357\n",
       "US-PA      486\n",
       "US-RI       10\n",
       "US-SC      173\n",
       "US-SD      162\n",
       "US-TN      228\n",
       "US-TX     1546\n",
       "US-U-A       3\n",
       "US-UT      103\n",
       "US-VA      311\n",
       "US-VT       66\n",
       "US-WA      379\n",
       "US-WI      457\n",
       "US-WV       83\n",
       "US-WY       95\n",
       "Name: iso_region, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking an overall look at iso_region\n",
    "airport_code_data.groupby('iso_region')['iso_region'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# removing US-U-A  - seems like a mistake\n",
    "airport_code_data = airport_code_data[airport_code_data.iso_region != 'US-U-A ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extracting the state code\n",
    "airport_code_data['state'] = airport_code_data['iso_region'].str.strip().str.split(\"-\", n = 1, expand = True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data['Country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reducing the size of the dataset\n",
    "temp_data = temp_data[temp_data['Country']=='Canada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# converting date into datetime\n",
    "temp_data['convertedDate'] = pd.to_datetime(temp_data.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#removing dates before the 1950 since there is not commercial airplanes before then\n",
    "temp_data=temp_data[temp_data['convertedDate']>\"1950-01-01\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               0\n",
       "AverageTemperature               0\n",
       "AverageTemperatureUncertainty    0\n",
       "City                             0\n",
       "Country                          0\n",
       "Latitude                         0\n",
       "Longitude                        0\n",
       "convertedDate                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "temp_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19100, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now making city and date as primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19100, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data[['City', 'convertedDate']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_dem_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at missing values\n",
    "city_dem_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Seems like we have the relevant information needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Wilmington</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>36.4</td>\n",
       "      <td>32680.0</td>\n",
       "      <td>39277.0</td>\n",
       "      <td>71957</td>\n",
       "      <td>3063.0</td>\n",
       "      <td>3336.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>DE</td>\n",
       "      <td>Asian</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>California</td>\n",
       "      <td>39.9</td>\n",
       "      <td>41523.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>81592</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>18274.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>24987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Glendale</td>\n",
       "      <td>California</td>\n",
       "      <td>42.1</td>\n",
       "      <td>98181.0</td>\n",
       "      <td>102844.0</td>\n",
       "      <td>201025</td>\n",
       "      <td>4448.0</td>\n",
       "      <td>111510.0</td>\n",
       "      <td>2.69</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "      <td>146718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Springfield</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>31.8</td>\n",
       "      <td>74744.0</td>\n",
       "      <td>79592.0</td>\n",
       "      <td>154336</td>\n",
       "      <td>5723.0</td>\n",
       "      <td>16226.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>MA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>5606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Bloomington</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>23.5</td>\n",
       "      <td>40588.0</td>\n",
       "      <td>43227.0</td>\n",
       "      <td>83815</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>10033.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>IN</td>\n",
       "      <td>Asian</td>\n",
       "      <td>9801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City          State  Median Age  Male Population  \\\n",
       "177   Wilmington       Delaware        36.4          32680.0   \n",
       "210     Lakewood     California        39.9          41523.0   \n",
       "238     Glendale     California        42.1          98181.0   \n",
       "300  Springfield  Massachusetts        31.8          74744.0   \n",
       "549  Bloomington        Indiana        23.5          40588.0   \n",
       "\n",
       "     Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "177            39277.0             71957              3063.0        3336.0   \n",
       "210            40069.0             81592              4094.0       18274.0   \n",
       "238           102844.0            201025              4448.0      111510.0   \n",
       "300            79592.0            154336              5723.0       16226.0   \n",
       "549            43227.0             83815              2368.0       10033.0   \n",
       "\n",
       "     Average Household Size State Code                Race   Count  \n",
       "177                    2.45         DE               Asian    1193  \n",
       "210                    3.13         CA  Hispanic or Latino   24987  \n",
       "238                    2.69         CA               White  146718  \n",
       "300                    2.81         MA               Asian    5606  \n",
       "549                    2.33         IN               Asian    9801  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# could we use city name and race as primary key?\n",
    "city_dem_data[city_dem_data[['City','Race']].duplicated()].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Seems like using City and Race isn't enough, perhaps adding State would make more sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure there are no duplicate rows\n",
    "city_dem_data[city_dem_data[['City','Race', 'State']].duplicated()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_data.createOrReplaceTempView(\"immigration_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              3096313|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#can cicid be used a primary key?\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT (DISTINCT cicid)\n",
    "FROM immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Seems like cicid is a good candidate for primary key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "All dates in SAS correspond to the number of days since 1960-01-01. We need to compute the arrival dates and departure dates by adding dates to 1960-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_data = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM immigration_table\")\n",
    "imm_data.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS departure_date \n",
    "                        \n",
    "                FROM immigration_table\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for no more null values\n",
    "spark.sql(\"SELECT count(*) FROM immigration_table WHERE departure_date = 'N/A'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Replacing I94VISA columns into the three categories where 1 = Business, 2 = Pleasure and 3 = Student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN i94visa = 1.0 THEN 'Business' \n",
    "                        WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                        WHEN i94visa = 3.0 THEN 'Student'\n",
    "                        ELSE 'N/A' END AS visa_type \n",
    "                        \n",
    "                FROM immigration_table\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT departure_date)|\n",
      "+------------------------------+\n",
      "|                            30|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge the common values between the two sets\n",
    "spark.sql(\"\"\"   SELECT COUNT(DISTINCT departure_date) \n",
    "                FROM immigration_table\n",
    "                WHERE departure_date IN (\n",
    "                    SELECT DISTINCT arrival_date FROM immigration_table\n",
    "                ) \n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Looking at different arrival modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94mode|count(1)|\n",
      "+-------+--------+\n",
      "|   null|     239|\n",
      "|    1.0| 2994505|\n",
      "|    3.0|   66660|\n",
      "|    2.0|   26349|\n",
      "|    9.0|    8560|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT i94mode, count(*)\n",
    "FROM immigration_table\n",
    "GROUP BY i94mode\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|max(biryear)|min(biryear)|\n",
      "+------------+------------+\n",
      "|      2019.0|      1902.0|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at birth year min and max\n",
    "spark.sql(\"SELECT MAX(biryear), MIN(biryear) FROM immigration_table WHERE biryear IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  326562|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(biryear=1902.0, count(1)=1),\n",
       " Row(biryear=1905.0, count(1)=1),\n",
       " Row(biryear=1906.0, count(1)=1),\n",
       " Row(biryear=1907.0, count(1)=2),\n",
       " Row(biryear=1908.0, count(1)=2),\n",
       " Row(biryear=1909.0, count(1)=1),\n",
       " Row(biryear=1911.0, count(1)=2),\n",
       " Row(biryear=1913.0, count(1)=1),\n",
       " Row(biryear=1914.0, count(1)=4),\n",
       " Row(biryear=1915.0, count(1)=2),\n",
       " Row(biryear=1916.0, count(1)=24),\n",
       " Row(biryear=1917.0, count(1)=19),\n",
       " Row(biryear=1918.0, count(1)=26),\n",
       " Row(biryear=1919.0, count(1)=52),\n",
       " Row(biryear=1920.0, count(1)=46),\n",
       " Row(biryear=1921.0, count(1)=88),\n",
       " Row(biryear=1922.0, count(1)=104),\n",
       " Row(biryear=1923.0, count(1)=185),\n",
       " Row(biryear=1924.0, count(1)=241),\n",
       " Row(biryear=1925.0, count(1)=319),\n",
       " Row(biryear=1926.0, count(1)=463),\n",
       " Row(biryear=1927.0, count(1)=638),\n",
       " Row(biryear=1928.0, count(1)=884),\n",
       " Row(biryear=1929.0, count(1)=1204),\n",
       " Row(biryear=1930.0, count(1)=1594),\n",
       " Row(biryear=1931.0, count(1)=1999),\n",
       " Row(biryear=1932.0, count(1)=2500),\n",
       " Row(biryear=1933.0, count(1)=2965),\n",
       " Row(biryear=1934.0, count(1)=3784),\n",
       " Row(biryear=1935.0, count(1)=4629),\n",
       " Row(biryear=1936.0, count(1)=5635),\n",
       " Row(biryear=1937.0, count(1)=6813),\n",
       " Row(biryear=1938.0, count(1)=8019),\n",
       " Row(biryear=1939.0, count(1)=9340),\n",
       " Row(biryear=1940.0, count(1)=10897),\n",
       " Row(biryear=1941.0, count(1)=12305),\n",
       " Row(biryear=1942.0, count(1)=14198),\n",
       " Row(biryear=1943.0, count(1)=16238),\n",
       " Row(biryear=1944.0, count(1)=18559),\n",
       " Row(biryear=1945.0, count(1)=19988),\n",
       " Row(biryear=1946.0, count(1)=24891),\n",
       " Row(biryear=1947.0, count(1)=28451),\n",
       " Row(biryear=1948.0, count(1)=29576),\n",
       " Row(biryear=1949.0, count(1)=32063),\n",
       " Row(biryear=1950.0, count(1)=33667),\n",
       " Row(biryear=1951.0, count(1)=34141)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at people that are 70 years old and over\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM immigration_table\n",
    "WHERE biryear IS NOT NULL\n",
    "AND biryear <= 1951\n",
    "\"\"\").show()\n",
    "\n",
    "#freq per birth year\n",
    "spark.sql(\"\"\"\n",
    "SELECT biryear, COUNT(*)\n",
    "FROM immigration_table \n",
    "WHERE biryear IS NOT NULL\n",
    "AND biryear <= 1951\n",
    "GROUP BY biryear\n",
    "ORDER BY biryear ASC\n",
    "\"\"\").head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     F| 1302743|\n",
      "|  null|  414269|\n",
      "|     M| 1377224|\n",
      "|     U|     467|\n",
      "|     X|    1610|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at gender\n",
    "spark.sql(\"\"\"\n",
    "SELECT gender, count(*) \n",
    "FROM immigration_table\n",
    "GROUP BY gender\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# removing missing or incorrect gender\n",
    "spark.sql(\"\"\"SELECT * FROM immigration_table WHERE gender IN ('F', 'M')\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  129668|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at citizenship and residence data for missing data\n",
    "\n",
    "#citizenship countries\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM immigration_table\n",
    "WHERE i94cit IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "#residence countries\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM immigration_table\n",
    "WHERE i94res IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "#reported address\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM immigration_table\n",
    "WHERE i94addr IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Address is practically unusable. Relying on entry port instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|occup|      n|\n",
      "+-----+-------+\n",
      "| null|2671843|\n",
      "|  STU|   4719|\n",
      "|  OTH|    661|\n",
      "|  NRR|    345|\n",
      "|  MKT|    280|\n",
      "|  EXA|    196|\n",
      "|  GLS|    189|\n",
      "|  ULS|    175|\n",
      "|  ADM|    125|\n",
      "|  TIE|    124|\n",
      "|  MVC|    110|\n",
      "|  ENO|     60|\n",
      "|  CEO|     56|\n",
      "|  TIP|     52|\n",
      "|  RET|     50|\n",
      "|  CMP|     47|\n",
      "|  LLJ|     46|\n",
      "|  PHS|     45|\n",
      "|  UNP|     45|\n",
      "|  HMK|     40|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at occupation\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT occup, COUNT(*) AS n\n",
    "FROM immigration_table\n",
    "GROUP BY occup\n",
    "ORDER BY n DESC, occup\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are lots of missing information. Not usable so we will be dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#building our immigration table\n",
    "df_immigration = spark.sql(\"\"\"SELECT * FROM immigration_table\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Since I'm interested in the flow of travellers through the united states. The i94 data will serve as our fact table. The fact_immigration table will be :\n",
    "\n",
    "* cicid,\n",
    "* citizenship_country,\n",
    "* residence_country,\n",
    "* city,\n",
    "* state,\n",
    "* arrival_date,\n",
    "* departure_date,\n",
    "* age,\n",
    "* visa_type,\n",
    "* detailed_visa_type,\n",
    "\n",
    "For the dimension tables, since our dataset only contains one month of data we will keep a record of the daily entries and provide the uses with four dimensions to aggregate our data:\\\n",
    "\n",
    "dim_time : to aggregate the data suing various time units: The fileds available will be:\n",
    "\n",
    "* date,\n",
    "* year,\n",
    "* month,\n",
    "* day,\n",
    "* week,\n",
    "* weekday,\n",
    "* dayofyear\n",
    "\n",
    "dim_airports: Used to determine the areas with the largest flow of travelers. Fileds included will be:\n",
    "\n",
    "* ident,\n",
    "* type,\n",
    "* name,\n",
    "* elevation_ft,\n",
    "* state,\n",
    "* municipality,\n",
    "* iata_code\n",
    "\n",
    "\n",
    "dim_city_demographics: To look at the demographic data of the areas with the most travelers and potentially look at the impact of the flow of travellers on the demographic data (if it were updated on a regular basis). The fiels available will be:\n",
    "\n",
    "* City,\n",
    "* state,\n",
    "* median_age,\n",
    "* male_population,\n",
    "* female_population,\n",
    "* total population\n",
    "* Foreign_born,\n",
    "* Average_Household_Size,\n",
    "* Race,\n",
    "* Count,\n",
    "\n",
    "dim_temperatures: to look at the temperature data of the cities where traveller entry and departure is being reported. The fields included will be:\n",
    "\n",
    "* date,\n",
    "* City,\n",
    "* average temperature,\n",
    "* average temperature uncertainty\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The steps necessary to pipeline the data into the chosen data model:\n",
    "\n",
    "1. Data Extraction: Load all the datasets from CSV and SAS data files;\n",
    "2. Data Transformation and Loading:\n",
    "\n",
    "    * fact_immigration:\n",
    "        * Drop rows where the mode of arrival is not air travel;\n",
    "        * Drop rows with incorrect gender data;\n",
    "        * Convert arrival and departure dates;\n",
    "        * Replace country codes with the character string equivalents;\n",
    "        * Replace port of entry with city and state;\n",
    "        * Filter out any row where the port of entry is not in the US;\n",
    "        * Compute age in a new row using birth year and year of our current date;\n",
    "        * insert data into fact table;\n",
    "        * Write to parquet.\n",
    "        \n",
    "    * dim_temperature:\n",
    "        * For the temperature table, drop all data for cities outside the united states;\n",
    "        * For the temperature table, drop all data for dates before 1950 since airtravel wasn't possible before that date;\n",
    "        * Convert city to upper case;\n",
    "        * Write to parquet.\n",
    "        \n",
    "    * dim_time:\n",
    "        * Get all the arrival dates from the immigration data_set;\n",
    "        * Extract year, month, day, week from the date and insert all the values in the dim_time table;\n",
    "        * Write to parquet.\n",
    "        \n",
    "    * dim_airports:\n",
    "        * Remove all non us airports;\n",
    "        * Remove all invalid port of entries, ie: ['closed', 'heliport', 'seaplane_base', 'balloonport'];\n",
    "        * Remove all rows where municipalities are missing;\n",
    "        * Convert municipality to upper case;\n",
    "        * Insert to table;\n",
    "        * Write to parquet.\n",
    "        \n",
    "   * dim_city_demographics:\n",
    "        * Convert to city names to upper case;\n",
    "        * Insert to table;\n",
    "        * Write to parquet.\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "Start off by reading the csv files using Pandas then convert them to spark dataframes. The reason why its being done this way, is because Spark automatically reads the fields in csv files as strings, whereas pandas automatically detects the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: double (nullable = true)\n",
      " |-- Female Population: double (nullable = true)\n",
      " |-- Total Population: long (nullable = true)\n",
      " |-- Number of Veterans: double (nullable = true)\n",
      " |-- Foreign-born: double (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(city_dem_data).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###  1. Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load dictionary data\n",
    "df_countryCodes = pd.read_csv('countries.csv')\n",
    "df_i94portCodes = pd.read_csv('I94-airport.csv')\n",
    "\n",
    "# load the various csv files into pandas dataframes\n",
    "df_demographics = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "df_temperature = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "\n",
    "# load the SAS data\n",
    "df_immigration=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert data dictionary to spark df to perform SQL queries\n",
    "spark_df_countryCodes = spark.createDataFrame(df_countryCodes)\n",
    "spark_df_countryCodes .createOrReplaceTempView(\"countryCodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove all entries with null values as they are either un reported or outside the US\n",
    "df_i94portCodes = df_i94portCodes[~df_i94portCodes.state.isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# exclude values for airports outside of the US. \n",
    "nonUSstates = ['CANADA', 'Canada', 'NETHERLANDS', 'NETH ANTILLES', 'THAILAND', 'ETHIOPIA', 'PRC', 'BERMUDA', 'COLOMBIA', 'ARGENTINA', 'MEXICO', \n",
    "               'BRAZIL', 'URUGUAY', 'IRELAND', 'GABON', 'BAHAMAS', 'MX', 'CAYMAN ISLAND', 'SEOUL KOREA', 'JAPAN', 'ROMANIA', 'INDONESIA',\n",
    "               'SOUTH AFRICA', 'ENGLAND', 'KENYA', 'TURK & CAIMAN', 'PANAMA', 'NEW GUINEA', 'ECUADOR', 'ITALY', 'EL SALVADOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94portCodes = df_i94portCodes[~df_i94portCodes.state.isin(nonUSstates)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# airport code spark df\n",
    "spark_df_i94portCodes = spark.createDataFrame(df_i94portCodes)\n",
    "spark_df_i94portCodes .createOrReplaceTempView(\"i94portCodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# immagration spark df\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove all entries into the united states that weren't via air travel\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM immigration_table\n",
    "WHERE i94mode = 1\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows where the gender values entered is undefined\n",
    "spark.sql(\"\"\"SELECT * FROM immigration_table WHERE gender IN ('F', 'M')\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the arrival dates into a useable value\n",
    "spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM immigration_table\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the departure dates into a useable value\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS departure_date \n",
    "                        \n",
    "                FROM immigration_table\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use an inner join to drop invalid codes\n",
    "#country of citizenship\n",
    "spark.sql(\"\"\"\n",
    "SELECT im.*, cc.country AS citizenship_country\n",
    "FROM immigration_table im\n",
    "INNER JOIN countryCodes cc\n",
    "ON im.i94cit = cc.code\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# country of residence\n",
    "spark.sql(\"\"\"\n",
    "SELECT im.*, cc.country AS residence_country\n",
    "FROM immigration_table im\n",
    "INNER JOIN countryCodes cc\n",
    "ON im.i94res = cc.code\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add visa character string aggregation\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN i94visa = 1.0 THEN 'Business' \n",
    "                        WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                        WHEN i94visa = 3.0 THEN 'Student'\n",
    "                        ELSE 'N/A' END AS visa_type \n",
    "                        \n",
    "                FROM immigration_table\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add entry_port names and entry port states to the view\n",
    "spark.sql(\"\"\"\n",
    "SELECT im.*, pc.location AS entry_port, pc.state AS entry_port_state\n",
    "FROM immigration_table im \n",
    "INNER JOIN i94portCodes pc\n",
    "ON im.i94port = pc.code\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compute the age of each individual and add it to the view\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, (2016-biryear) AS age \n",
    "FROM immigration_table\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Insert the immigration fact data into a spark dataframe\n",
    "fact_immigration = spark.sql(\"\"\"\n",
    "                        SELECT \n",
    "                            cicid, \n",
    "                            citizenship_country,\n",
    "                            residence_country,\n",
    "                            TRIM(UPPER (entry_port)) AS city,\n",
    "                            TRIM(UPPER (entry_port_state)) AS state,\n",
    "                            arrival_date,\n",
    "                            departure_date,\n",
    "                            age,\n",
    "                            visa_type,\n",
    "                            visatype AS detailed_visa_type\n",
    "\n",
    "                        FROM immigration_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract all distinct dates from arrival and departure dates to create dimension table\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT arrival_date AS date\n",
    "FROM immigration_table\n",
    "UNION\n",
    "SELECT DISTINCT departure_date AS date\n",
    "FROM immigration_table\n",
    "WHERE departure_date IS NOT NULL\n",
    "\"\"\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract year, month, day, weekofyear, dayofweek and weekofyear from the date and insert all the values in the dim_time table;\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT date, YEAR(date) AS year, MONTH(date) AS month, DAY(date) AS day, WEEKOFYEAR(date) AS week, DAYOFWEEK(date) as weekday, DAYOFYEAR(date) year_day\n",
    "FROM dim_time_table\n",
    "ORDER BY date ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep only data for the United States\n",
    "df_temperature = df_temperature[df_temperature['Country']=='United States'].copy()\n",
    "\n",
    "# Convert the date to datetime objects\n",
    "df_temperature['date'] = pd.to_datetime(df_temperature.dt)\n",
    "\n",
    "# Remove all dates prior to 1950\n",
    "df_temperature=df_temperature[df_temperature['date']>\"1950-01-01\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the city names to upper case\n",
    "df_temperature.City = df_temperature.City.str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the dataframes from pandas to spark\n",
    "spark_df_temperature = spark.createDataFrame(df_temperature)\n",
    "spark_df_temperature .createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dim_temperature = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DISTINCT date, city,\n",
    "    AVG(AverageTemperature) OVER (PARTITION BY date, City) AS average_temperature, \n",
    "    AVG(AverageTemperatureUncertainty)  OVER (PARTITION BY date, City) AS average_termperature_uncertainty\n",
    "    \n",
    "FROM temperature\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.City = df_demographics.City.str.strip().str.upper()\n",
    "df_demographics['State Code'] = df_demographics['State Code'].str.strip().str.upper()\n",
    "df_demographics.Race = df_demographics.Race.str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the dataframes from pandas to spark\n",
    "spark_df_demographics = spark.createDataFrame(df_demographics)\n",
    "spark_df_demographics.createOrReplaceTempView(\"demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# insert data into the demographics dim table\n",
    "dim_demographics = spark.sql(\"\"\"\n",
    "                                SELECT  City, \n",
    "                                        State, \n",
    "                                        `Median Age` AS median_age, \n",
    "                                        `Male Population` AS male_population, \n",
    "                                        `Female Population` AS female_population, \n",
    "                                        `Total Population` AS total_population, \n",
    "                                        `Foreign-born` AS foreign_born, \n",
    "                                        `Average Household Size` AS average_household_size, \n",
    "                                        `State Code` AS state_code, \n",
    "                                        Race, \n",
    "                                        Count\n",
    "                                FROM demographics\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# airport dataset contains a lot of nulls. We'll load the csv directly into a spark dataframe to avoid having to deal with converting pandas NaN into nulls\n",
    "spark_df_airports = spark.read.format(\"csv\").option(\"header\", \"true\").load('airport-codes_csv.csv')\n",
    "spark_df_airports.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get country as US and not-null\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM airports\n",
    "WHERE iso_country IS NOT NULL\n",
    "AND UPPER(TRIM(iso_country)) LIKE 'US'\n",
    "\"\"\").createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# further cleaning of airports\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM airports\n",
    "WHERE LOWER(TRIM(type)) NOT IN ('closed', 'heliport', 'seaplane_base', 'balloonport')\n",
    "AND municipality IS NOT NULL\n",
    "AND LENGTH(iso_region) = 5\n",
    "\"\"\").createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airports = spark.sql(\"\"\"\n",
    "SELECT TRIM(ident) AS ident, type, name, elevation_ft, SUBSTR(iso_region, 4) AS state, TRIM(UPPER(municipality)) AS municipality, iata_code\n",
    "FROM airports\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "dim_demographics.write.parquet(\"dim_demographics\")\n",
    "dim_time.write.parquet(\"dim_time\")\n",
    "dim_airports.write.parquet(\"dim_airports\")\n",
    "dim_temperature.write.parquet(\"dim_temperature\")\n",
    "fact_immigration.write.parquet(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# retrieving our data\n",
    "dim_demographics.createOrReplaceTempView(\"dim_demographics\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")\n",
    "dim_airports.createOrReplaceTempView(\"dim_airports\")\n",
    "dim_temperature.createOrReplaceTempView(\"dim_temperature\")\n",
    "fact_immigration.createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check for null values function\n",
    "\n",
    "def nullValueCheck(spark_ctxt, tables_to_check):\n",
    "    \"\"\"\n",
    "    This function performs null value checks on specific columns of given tables received as parameters and raises a ValueError exception when null values are encountered.\n",
    "    It receives the following parameters:\n",
    "    spark_ctxt: spark context where the data quality check is to be performed\n",
    "    tables_to_check: A dictionary containing (table, columns) pairs specifying for each table, which column is to be checked for null values.   \n",
    "    \"\"\"  \n",
    "    for table in tables_to_check:\n",
    "        print(f\"Performing data quality check on table {table}...\")\n",
    "        for column in tables_to_check[table]:\n",
    "            returnedVal = spark_ctxt.sql(f\"\"\"SELECT COUNT(*) as nbr FROM {table} WHERE {column} IS NULL\"\"\")\n",
    "            if returnedVal.head()[0] > 0:\n",
    "                raise ValueError(f\"Data quality check failed! Found NULL values in {column} column!\")\n",
    "        print(f\"Table {table} passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|(count(1) - CAST(192 AS BIGINT))|\n",
      "+--------------------------------+\n",
      "|                               0|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------------------------+\n",
      "|(count(DISTINCT date) - CAST(192 AS BIGINT))|\n",
      "+--------------------------------------------+\n",
      "|                                           0|\n",
      "+--------------------------------------------+\n",
      "\n",
      "+----+\n",
      "|date|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time dimension verification\n",
    "\n",
    "#check the number of rows in our time table : 192 expected\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) - 192\n",
    "FROM dim_time\n",
    "\"\"\").show()\n",
    "\n",
    "# make sure each row has a distinct date key : 192 expected\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT date) - 192\n",
    "FROM dim_time\n",
    "\"\"\").show()\n",
    "\n",
    "# we could also subtract the result of one query from the other\n",
    "\n",
    "\n",
    "# and make sure all dates from the fact table are included in the time dimension (NULL is the expected result)\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT date\n",
    "FROM dim_time\n",
    "\n",
    "MINUS\n",
    "\n",
    "(SELECT DISTINCT arrival_date AS date\n",
    "FROM immigration_table\n",
    "UNION\n",
    "SELECT DISTINCT departure_date AS date\n",
    "FROM immigration_table\n",
    "WHERE departure_date IS NOT NULL)\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|(count(DISTINCT cicid) - CAST(2165257 AS BIGINT))|\n",
      "+-------------------------------------------------+\n",
      "|                                                0|\n",
      "+-------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------+\n",
      "|(count(DISTINCT cicid) - CAST(2165257 AS BIGINT))|\n",
      "+-------------------------------------------------+\n",
      "|                                                0|\n",
      "+-------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|(count(1) - CAST(2165257 AS BIGINT))|\n",
      "+------------------------------------+\n",
      "|                                   0|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#immigration verification\n",
    "\n",
    "# The number of primary key from the staging table (2165257 expected)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(distinct cicid) - 2165257\n",
    "FROM immigration_table\n",
    "\"\"\").show()\n",
    "\n",
    "#should match the primary key count from the fact table (2165257 expected)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(distinct cicid) - 2165257\n",
    "FROM fact_immigration\n",
    "\"\"\").show()\n",
    "\n",
    "#and should match the row count from the fact table since it is also the primary key (2165257 expected)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 2165257\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|(count(1) - CAST(2891 AS BIGINT))|\n",
      "+---------------------------------+\n",
      "|                                0|\n",
      "+---------------------------------+\n",
      "\n",
      "+----------------------------------------------------------+\n",
      "|(count(DISTINCT city, state, race) - CAST(2891 AS BIGINT))|\n",
      "+----------------------------------------------------------+\n",
      "|                                                         0|\n",
      "+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demographics dimension table (2891 expected) \n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 2891\n",
    "FROM dim_demographics\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT city, state, race) - 2891\n",
    "FROM dim_demographics\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|(count(1) - CAST(14529 AS BIGINT))|\n",
      "+----------------------------------+\n",
      "|                                 0|\n",
      "+----------------------------------+\n",
      "\n",
      "+-----------------------------------------------+\n",
      "|(count(DISTINCT ident) - CAST(14529 AS BIGINT))|\n",
      "+-----------------------------------------------+\n",
      "|                                              0|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# primary key for airports (expected 14529)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 14529\n",
    "FROM dim_airports\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT ident) - 14529\n",
    "FROM dim_airports\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|(count(1) - CAST(189472 AS BIGINT))|\n",
      "+-----------------------------------+\n",
      "|                                  0|\n",
      "+-----------------------------------+\n",
      "\n",
      "+-----------------------------------------------------+\n",
      "|(count(DISTINCT date, city) - CAST(189472 AS BIGINT))|\n",
      "+-----------------------------------------------------+\n",
      "|                                                    0|\n",
      "+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# city & date is our primary key for the temperature (expected 189472)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 189472\n",
    "FROM dim_temperature\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT date, city) - 189472\n",
    "FROM dim_temperature\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "Please see the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Considering the size of the immigration dataset (~ 3 million rows) for only a month, combined with the temperature, airport and demographic dataset, the most sensible technology choice for such an approach would be spark, especially for processing the data over a longer period of time.\n",
    "\n",
    "This project was intended to look for:\n",
    "\n",
    "* the effects of temperature aon the volume of travellers,\n",
    "* the seasonality of travel\n",
    "* the connection between the volume of travel and the number of entry ports (ie airports)\n",
    "* the connection between the volume of travel and the demographics of various cities\n",
    "\n",
    "None of these phenomenons require a rapid update of our data. A monthly or quarterly update is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### If under the following scenarios:\n",
    "    * Data is increased by 100x: Use S3 bucket storage and still use spark as the processing platform (since its well suited for large datasets);\n",
    "    * Dashboard needs to be updated daily: Use apache Airflow to perform the ETL and data quality validation;\n",
    "    * database needs to be accessed by >100 people; once the data has been processed, it can be stored in a postgres database (using Redshift), which supports multiuser access."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
