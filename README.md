# Data on immigration to the US

The purpose of this data engineering capstone project is to combine what was learned in the Data Engineer Nanodegree program at Udacity. 

Here are the following steps taken to complete this project:

Step 1: Scope the Project and Gather Data
- Identify and gather the data (of at least two sources and more than 1 million rows);
- Explain what end use cases to prepare the data for (e.g., analytics table, app back-end, source-of-truth database, etc.)

Step 2: Explore and Assess the Data
- Explore the data to identify data quality issues, like missing values, duplicate data, etc;
- Document steps necessary to clean the data


Step 3: Define the Data Model
- Map out the conceptual data model;
- List the steps necessary to pipeline the data.

Step 4: Run ETL to Model the Data
- Create the data pipelines and the data model;
- Include a data dictionary;
- Run data quality checks to ensure the pipeline ran as expected;
- Integrity constraints on the relational database (e.g., unique key, data type, etc.);
- Unit tests for the scripts to ensure they are doing the right thing.

Step 5: Complete Project Write Up
- Document the steps of the process.
- Propose how often the data should be updated and why.
- Include a description of how you would approach the problem differently under the following scenarios:
  - If the data was increased by 100x.
  - If the pipelines were run on a daily basis by 7am.
  - If the database needed to be accessed by 100+ people.
